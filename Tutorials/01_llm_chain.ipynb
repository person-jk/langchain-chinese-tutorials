{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建一个简单的 LLM 应用程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此快速入门中，我们将向您展示如何构建一个简单的 LLM 应用程序。\n",
    "\n",
    "此应用程序将把文本从中文翻译成另一种语言。\n",
    "\n",
    "这是一个相对简单的 LLM 应用程序 - 它只是一个 LLM 调用加上一些提示。\n",
    "\n",
    "不过，这仍然是开始使用 LangChain 的好方法 - 只需一些提示和 LLM 调用就可以构建许多功能！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 概念\n",
    "我们将介绍的概念包括：\n",
    "\n",
    "1. 使用语言模型\n",
    "\n",
    "2. 使用 PromptTemplates 和 OutputParsers\n",
    "\n",
    "4. 使用 LangChain 链接 PromptTemplate + LLM + OutputParser\n",
    "\n",
    "5. 使用 LangSmith 调试和跟踪您的应用程序\n",
    "\n",
    "6. 使用 LangServe 部署您的应用程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本次将使用Jupyter Notebook\n",
    "\n",
    "Jupyter Notebook\n",
    "本指南（以及文档中的大多数其他指南）使用 Jupyter Notebook，并假设读者也使用 Jupyter Notebook。Jupyter Notebook 非常适合学习如何使用 LLM 系统，\n",
    "\n",
    "因为很多时候可能会出错（意外输出、API 故障等），\n",
    "而在交互式环境中阅读指南是更好地理解它们的好方法。\n",
    "\n",
    "本教程和其他教程可能最方便在 Jupyter Notebook 中运行。\n",
    "\n",
    "请参阅此处了解如何安装的说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安装langchain \n",
    "!pip install langchain==0.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用LangSmith\n",
    "国内网络搞不了，我们就不搞这个了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然如果你可以，那你用吧。\n",
    "主要就是用来看 langchin调用链，debug是真的方便。\n",
    "\n",
    "\n",
    "您使用 LangChain 构建的许多应用程序将包含多个步骤，并多次调用 LLM 调用。随着这些应用程序变得越来越复杂，能够检查链或代理内部究竟发生了什么变得至关重要。最好的方法是使用 LangSmith。\n",
    "\n",
    "在上面的链接处注册后，请确保设置环境变量以开始记录跟踪:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用语言模型\n",
    "首先，让我们学习如何单独使用语言模型。LangChain 支持许多不同的语言模型，您可以互换使用 - \n",
    "\n",
    "在下面选择您想要使用的模型！ 选智谱就好了，其他你们想用就用其他的吧\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_zhipu==4.1.5  # 怎么注册智谱官方怎么拿apikey，我这边就不多说了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"ZHIPUAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_zhipu import ChatZhipuAI\n",
    "\n",
    "model = ChatZhipuAI(model=\"glm-3-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们首先直接使用该模型。\n",
    "ChatZhipuAI 是 LangChain“Runnable”的实例，这意味着它们公开了一个用于与其交互的标准接口。\n",
    "为了简单地调用该模型，我们可以将消息列表传递给 <code>.invoke </code>方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', response_metadata={'id': '8705678841153504326', 'created': 1717085203, 'token_usage': {'completion_tokens': 5, 'prompt_tokens': 18, 'total_tokens': 23}, 'model_name': 'glm-3-turbo', 'finish_reason': 'stop'}, id='run-e0a6a26d-127b-47d2-b9fd-c495e59bb746-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"将以下内容从中文翻译成意大利语\"),\n",
    "    HumanMessage(content=\"你好！\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们启用了 LangSmith， 我们可以看到这次运行被记录到 LangSmith，并且可以看到 LangSmith 的跟踪 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OutputParsers 输出解析器\n",
    "请注意，模型的响应是 AIMessage。它包含字符串响应以及有关响应的其他元数据。\n",
    "\n",
    "通常我们可能只想处理字符串响应。我们可以使用简单的输出解析器来解析此响应。\n",
    "\n",
    "我们首先导入简单的输出解析器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种使用方法是直接使用它。例如，我们可以保存语言模型调用的结果，然后将其传递给解析器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更常见的是，我们可以将模型与此输出解析器“链接”在一起。这意味着此输出解析器将在此链中每次被调用。\n",
    "\n",
    "此链采用语言模型的输入类型（字符串或消息列表）并返回输出解析器的输出类型（字符串）。\n",
    "\n",
    "我们可以使用 | 运算符轻松创建链。| 运算符在 LangChain 中用于将两个元素组合在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们现在看看 LangSmith，我们可以看到该链有两个步骤：首先调用语言模型，然后将结果传递给输出解析器。我们可以看到 LangSmith 跟踪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates  提示词模板"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们将消息列表直接传递到语言模型中。这个消息列表从何而来？\n",
    "\n",
    "通常，它由用户输入和应用程序逻辑组合而成。\n",
    "\n",
    "此应用程序逻辑通常获取原始用户输入并将其转换为可传递给语言模型的消息列表。\n",
    "\n",
    "常见的转换包括添加系统消息或使用用户输入格式化模板。\n",
    "\n",
    "PromptTemplates 是 LangChain 中的一个概念，旨在协助进行这种转换。它们接收原始用户输入并返回可传递给语言模型的数据（提示）。\n",
    "\n",
    "让我们在这里创建一个 PromptTemplate。它将接收两个用户变量：\n",
    "\n",
    "`language`：要将文本翻译成的语言\n",
    "\n",
    "`text`：要翻译的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先引入包\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，让我们创建一个字符串，将其格式化为系统消息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"将以下内容翻译成 {language}:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们可以创建 PromptTemplate。这将是 system_template 的组合，以及用于放置文本的更简单的模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此提示模板的输入是个数组。我们可以单独试用此提示模板，看看它能不能翻译！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='将以下内容翻译成 意大利:'), HumanMessage(content='你好')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt_template.invoke({\"language\": \"意大利\", \"text\": \"你好\"})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到它返回了一个由两条消息组成的 ChatPromptValue。如果我们想直接访问这些消息，我们可以这样做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='将以下内容翻译成 意大利:'), HumanMessage(content='你好')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以将其与上面的模型 和 输出解析器结合起来。这将把​​所有三个组件链接在一起。也就是LECL。\n",
    "\n",
    "也就是将链路修改为： 提示词模板 -> 大模型调用 -> 整理输出的消息格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"language\": \"意大利\", \"text\": \"你好\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们查看 LangSmith 跟踪，我们可以看到所有三个组件都出现在 LangSmith 跟踪中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 LangServe 提供服务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经构建了一个应用程序，我们需要为它提供服务。这就是 LangServe 的作用所在。\n",
    "\n",
    "LangServe 帮助开发人员将 LangChain 链部署为 REST API。\n",
    "\n",
    "您不需要使用 LangServe 来使用 LangChain，但在本指南中，我们将展示如何使用 LangServe 部署您的应用程序。\n",
    "\n",
    "虽然本指南的第一部分旨在在 Jupyter Notebook 或脚本中运行，但现在我们将不再这样做。我们将创建一个 Python 文件，然后从命令行与其交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是使用langServe是不能用 Jupyter Notebook来运行了，要写个py文件，然后python运行。 \n",
    "\n",
    "如果你熟悉fastapi就非常好理解了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Collecting langserve[all]\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/97/92/ab9bc89750ee947b89e3c0b880ae39b24e05257f175c375bdfe963f02c35/langserve-0.2.1-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.1/1.2 MB 1.6 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 0.1/1.2 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.3/1.2 MB 2.4 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 0.6/1.2 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.2/1.2 MB 5.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fastapi<1,>=0.90.1 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from langserve[all]) (0.110.3)\n",
      "Requirement already satisfied: httpx>=0.23.0 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from langserve[all]) (0.27.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from langserve[all]) (0.2.1)\n",
      "Requirement already satisfied: orjson>=2 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from langserve[all]) (3.10.2)\n",
      "Requirement already satisfied: pydantic>=1 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from langserve[all]) (2.7.1)\n",
      "Collecting pyproject-toml<0.0.11,>=0.0.10 (from langserve[all])\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3b/56/95d700e725946200ec9b2aeee4479fcf3ca24cf6fbf0aa548160980787a5/pyproject_toml-0.0.10-py3-none-any.whl (6.9 kB)\n",
      "Collecting sse-starlette<2.0.0,>=1.3.0 (from langserve[all])\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/34/7e/d6087916bf58a4343459b47807a116a3a755e6ddd4857f375547e00f6252/sse_starlette-1.8.2-py3-none-any.whl (8.9 kB)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from fastapi<1,>=0.90.1->langserve[all]) (0.37.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from fastapi<1,>=0.90.1->langserve[all]) (4.11.0)\n",
      "Requirement already satisfied: anyio in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from httpx>=0.23.0->langserve[all]) (3.7.1)\n",
      "Requirement already satisfied: certifi in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from httpx>=0.23.0->langserve[all]) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from httpx>=0.23.0->langserve[all]) (1.0.5)\n",
      "Requirement already satisfied: idna in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from httpx>=0.23.0->langserve[all]) (3.7)\n",
      "Requirement already satisfied: sniffio in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from httpx>=0.23.0->langserve[all]) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->langserve[all]) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from langchain-core<0.3,>=0.1->langserve[all]) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from langchain-core<0.3,>=0.1->langserve[all]) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from langchain-core<0.3,>=0.1->langserve[all]) (0.1.56)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from langchain-core<0.3,>=0.1->langserve[all]) (23.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from langchain-core<0.3,>=0.1->langserve[all]) (8.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from pydantic>=1->langserve[all]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from pydantic>=1->langserve[all]) (2.18.2)\n",
      "Requirement already satisfied: setuptools>=42 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from pyproject-toml<0.0.11,>=0.0.10->langserve[all]) (68.2.2)\n",
      "Requirement already satisfied: wheel in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from pyproject-toml<0.0.11,>=0.0.10->langserve[all]) (0.41.2)\n",
      "Requirement already satisfied: toml in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from pyproject-toml<0.0.11,>=0.0.10->langserve[all]) (0.10.2)\n",
      "Requirement already satisfied: jsonschema in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from pyproject-toml<0.0.11,>=0.0.10->langserve[all]) (4.22.0)\n",
      "Requirement already satisfied: uvicorn in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from sse-starlette<2.0.0,>=1.3.0->langserve[all]) (0.25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1->langserve[all]) (2.4)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1->langserve[all]) (2.31.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from jsonschema->pyproject-toml<0.0.11,>=0.0.10->langserve[all]) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from jsonschema->pyproject-toml<0.0.11,>=0.0.10->langserve[all]) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from jsonschema->pyproject-toml<0.0.11,>=0.0.10->langserve[all]) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from jsonschema->pyproject-toml<0.0.11,>=0.0.10->langserve[all]) (0.18.1)\n",
      "Requirement already satisfied: click>=7.0 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from uvicorn->sse-starlette<2.0.0,>=1.3.0->langserve[all]) (8.1.7)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from click>=7.0->uvicorn->sse-starlette<2.0.0,>=1.3.0->langserve[all]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1->langserve[all]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\miniconda\\envs\\langchain\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1->langserve[all]) (2.2.1)\n",
      "Installing collected packages: sse-starlette, pyproject-toml, langserve\n",
      "Successfully installed langserve-0.2.1 pyproject-toml-0.0.10 sse-starlette-1.8.2\n"
     ]
    }
   ],
   "source": [
    "## 首先先安装这个langServe服务吧\n",
    "!pip install \"langserve[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编写py服务文件\n",
    "\n",
    "要为我们的应用程序创建服务器，我们将创建一个 serve.py 文件。\n",
    "\n",
    "它将包含为应用程序提供服务的逻辑。它由三部分组成：\n",
    "\n",
    "我们刚刚在上面构建的链的定义\n",
    "\n",
    "我们的 FastAPI 应用程序\n",
    "\n",
    "为链提供服务的路由的定义，这是通过 `langserve.add_routes` 完成的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意噢，下面那个是serve.py 文件的所有内容。 别在Jupyter Notebook 运行！会报错。\n",
    "\n",
    "你应该新建一个serve.py ，让把下面的内容复制进去！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意噢，这个是serve.py 文件的所有内容。 千万别在Jupyter Notebook 运行！\n",
    "#!/usr/bin/env python\n",
    "from typing import List\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_zhipu import ChatZhipuAI\n",
    "from langserve import add_routes\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"ZHIPUAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# 1. Create prompt template\n",
    "system_template = \"将以下内容翻译成 {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_template),\n",
    "    ('user', '{text}')\n",
    "])\n",
    "\n",
    "# 2. Create model\n",
    "model = ChatZhipuAI(model=\"glm-3-turbo\")\n",
    "\n",
    "\n",
    "# 3. Create parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4. Create chain\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "# 4. App definition\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 5. Adding chain route\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/chain\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么运行这个代码呢？   `python serve.py`\n",
    "\n",
    "注意，conda环境要切换且在这个目录下运行即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们打开 http://localhost:8000 就能看到啦！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground\n",
    "\n",
    "每个 LangServe 服务都带有一个简单的内置 UI，用于配置和调用应用程序，并提供流式输出和对中间步骤的可见性。\n",
    "\n",
    "前往 http://localhost:8000/chain/playground/ 试用！\n",
    "\n",
    "传入与之前相同的输入 - {\"language\": \"意大利\", \"text\": \"你好\"} - 它应该会像以前一样做出响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client 客户端调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们设置一个客户端，以便以编程方式与我们的服务进行交互。\n",
    "\n",
    "我们可以使用 [langserve.RemoteRunnable](/docs/langserve/#client) 轻松完成此操作。\n",
    "\n",
    "使用它，我们可以与服务链进行交互，就像它在客户端运行一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/chain/\")\n",
    "remote_chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结论\n",
    "就是这样！在本教程中，我们介绍了如何创建第一个简单的 LLM 应用程序。\n",
    "\n",
    "我们学习了如何使用语言模型、如何解析它们的输出、如何创建提示模板、如何将出色的可观察性融入使用 LangSmith 创建的链中以及如何使用 LangServe 部署它们。\n",
    "\n",
    "这只是您成为熟练的 AI 工程师所需学习内容的冰山一角。幸运的是 - 我们还有很多其他资源！\n",
    "\n",
    "有关更深入的教程，请查看我们的教程部分。\n",
    "\n",
    "如果您对如何完成特定任务有具体问题，请参阅我们的操作指南部分。\n",
    "\n",
    "要了解 LangChain 的核心概念，我们有详细的概念指南"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
